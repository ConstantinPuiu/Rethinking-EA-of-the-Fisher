{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3032,
     "status": "ok",
     "timestamp": 1648036996446,
     "user": {
      "displayName": "Constantin Puiu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "fpp4PfYe-5Ig",
    "outputId": "71937dc1-11b1-48f3-c13b-8da74cca2468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/Lib_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1648036997687,
     "user": {
      "displayName": "Constantin Puiu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "4xdyCPdJFrgK",
    "outputId": "b43a5995-5a94-4e0f-caf6-41c07c475057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1648036997690,
     "user": {
      "displayName": "Constantin Puiu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "s5rNPPlr2yd5",
    "outputId": "8beda52a-7b84-4138-eac8-cea0266346af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 54.8 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIO4m7tu-Mlr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 739134,
     "status": "ok",
     "timestamp": 1648037736813,
     "user": {
      "displayName": "Constantin Puiu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "APPMASzVCvNU",
    "outputId": "5c490cbf-54cf-4cb6-c4d9-38e8c08526f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After moving, parameters on device cuda:0\n",
      "After moving, parameters on device cuda:0\n",
      "After moving, parameters on device cuda:0\n",
      "After moving, parameters on device cuda:0\n",
      "After moving, parameters on device cuda:0\n",
      "After moving, parameters on device cuda:0\n",
      "After moving, parameters on device cuda:0\n",
      "After moving, parameters on device cuda:0\n",
      "the number of parameters is 4712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:125: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3092, Accuracy: 974/10000 (10%)\n",
      "\n",
      "\n",
      " {} \n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SO_KLD_WRM.py:215: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  p.grad.data.add_(self.weight_decay, p.data)\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SO_KLD_WRM.py:230: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.\n",
      "The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n",
      "L, _ = torch.symeig(A, upper=upper)\n",
      "should be replaced with\n",
      "L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n",
      "and\n",
      "L, V = torch.symeig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2499.)\n",
      "  self.m_aa[m], eigenvectors=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param norm: 8.763480186462402\n",
      "\n",
      "Test set: Avg. loss: 0.6893, Accuracy: 9499/10000 (95%)\n",
      "\n",
      "Took 14.997501373291016s\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.444764\n",
      "param norm: 13.960679054260254\n",
      "\n",
      "Test set: Avg. loss: 0.5298, Accuracy: 9518/10000 (95%)\n",
      "\n",
      "Took 14.831793069839478s\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.355930\n",
      "param norm: 15.1319580078125\n",
      "\n",
      "Test set: Avg. loss: 0.4628, Accuracy: 9629/10000 (96%)\n",
      "\n",
      "Took 14.362119674682617s\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.346994\n",
      "param norm: 15.807779312133789\n",
      "\n",
      "Test set: Avg. loss: 0.3852, Accuracy: 9651/10000 (97%)\n",
      "\n",
      "Took 14.651741027832031s\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.234567\n",
      "param norm: 16.206356048583984\n",
      "\n",
      "Test set: Avg. loss: 0.3484, Accuracy: 9671/10000 (97%)\n",
      "\n",
      "Took 14.740368843078613s\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.248638\n",
      "param norm: 16.552013397216797\n",
      "\n",
      "Test set: Avg. loss: 0.3546, Accuracy: 9698/10000 (97%)\n",
      "\n",
      "Took 14.588042736053467s\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.265514\n",
      "param norm: 16.837026596069336\n",
      "\n",
      "Test set: Avg. loss: 0.3455, Accuracy: 9637/10000 (96%)\n",
      "\n",
      "Took 14.604439973831177s\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.251535\n",
      "param norm: 17.066850662231445\n",
      "\n",
      "Test set: Avg. loss: 0.2931, Accuracy: 9759/10000 (98%)\n",
      "\n",
      "Took 14.668254137039185s\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.274086\n",
      "param norm: 17.259363174438477\n",
      "\n",
      "Test set: Avg. loss: 0.2928, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Took 14.636046648025513s\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.215833\n",
      "param norm: 17.391530990600586\n",
      "\n",
      "Test set: Avg. loss: 0.2844, Accuracy: 9753/10000 (98%)\n",
      "\n",
      "Took 14.639737844467163s\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.178742\n",
      "param norm: 17.55124282836914\n",
      "\n",
      "Test set: Avg. loss: 0.2788, Accuracy: 9759/10000 (98%)\n",
      "\n",
      "Took 14.758534669876099s\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.224620\n",
      "param norm: 17.659753799438477\n",
      "\n",
      "Test set: Avg. loss: 0.3033, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "Took 14.546461582183838s\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.236203\n",
      "param norm: 17.707473754882812\n",
      "\n",
      "Test set: Avg. loss: 0.2794, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Took 14.591299295425415s\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.185373\n",
      "param norm: 17.824628829956055\n",
      "\n",
      "Test set: Avg. loss: 0.2728, Accuracy: 9756/10000 (98%)\n",
      "\n",
      "Took 14.818603992462158s\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.175311\n",
      "param norm: 17.851064682006836\n",
      "\n",
      "Test set: Avg. loss: 0.2830, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Took 14.785007238388062s\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.192630\n",
      "param norm: 17.830120086669922\n",
      "\n",
      "Test set: Avg. loss: 0.2527, Accuracy: 9793/10000 (98%)\n",
      "\n",
      "Took 14.63230562210083s\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.169802\n",
      "param norm: 17.8392333984375\n",
      "\n",
      "Test set: Avg. loss: 0.2492, Accuracy: 9779/10000 (98%)\n",
      "\n",
      "Took 14.60346245765686s\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.203126\n",
      "param norm: 17.825008392333984\n",
      "\n",
      "Test set: Avg. loss: 0.2712, Accuracy: 9716/10000 (97%)\n",
      "\n",
      "Took 14.551514387130737s\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.163479\n",
      "param norm: 17.889583587646484\n",
      "\n",
      "Test set: Avg. loss: 0.2532, Accuracy: 9778/10000 (98%)\n",
      "\n",
      "Took 15.051838397979736s\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.211419\n",
      "param norm: 17.946565628051758\n",
      "\n",
      "Test set: Avg. loss: 0.2325, Accuracy: 9812/10000 (98%)\n",
      "\n",
      "Took 14.59349274635315s\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.196423\n",
      "param norm: 17.897140502929688\n",
      "\n",
      "Test set: Avg. loss: 0.2370, Accuracy: 9808/10000 (98%)\n",
      "\n",
      "Took 14.610334873199463s\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.160495\n",
      "param norm: 17.915693283081055\n",
      "\n",
      "Test set: Avg. loss: 0.2628, Accuracy: 9772/10000 (98%)\n",
      "\n",
      "Took 14.703088521957397s\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.187826\n",
      "param norm: 17.945415496826172\n",
      "\n",
      "Test set: Avg. loss: 0.2347, Accuracy: 9809/10000 (98%)\n",
      "\n",
      "Took 14.619324445724487s\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.180125\n",
      "param norm: 17.949934005737305\n",
      "\n",
      "Test set: Avg. loss: 0.2690, Accuracy: 9755/10000 (98%)\n",
      "\n",
      "Took 14.461518287658691s\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.207778\n",
      "param norm: 17.95949935913086\n",
      "\n",
      "Test set: Avg. loss: 0.2545, Accuracy: 9789/10000 (98%)\n",
      "\n",
      "Took 14.505985975265503s\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.151488\n",
      "param norm: 17.94544792175293\n",
      "\n",
      "Test set: Avg. loss: 0.2421, Accuracy: 9784/10000 (98%)\n",
      "\n",
      "Took 14.43104362487793s\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.190763\n",
      "param norm: 17.94017219543457\n",
      "\n",
      "Test set: Avg. loss: 0.2492, Accuracy: 9772/10000 (98%)\n",
      "\n",
      "Took 14.747092008590698s\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.153091\n",
      "param norm: 17.959999084472656\n",
      "\n",
      "Test set: Avg. loss: 0.2367, Accuracy: 9794/10000 (98%)\n",
      "\n",
      "Took 14.785805463790894s\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.178293\n",
      "param norm: 17.964675903320312\n",
      "\n",
      "Test set: Avg. loss: 0.2289, Accuracy: 9797/10000 (98%)\n",
      "\n",
      "Took 15.017600536346436s\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.171894\n",
      "param norm: 17.932743072509766\n",
      "\n",
      "Test set: Avg. loss: 0.2340, Accuracy: 9803/10000 (98%)\n",
      "\n",
      "Took 14.686250448226929s\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.175325\n",
      "param norm: 17.965251922607422\n",
      "\n",
      "Test set: Avg. loss: 0.2267, Accuracy: 9813/10000 (98%)\n",
      "\n",
      "Took 14.708582162857056s\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.188839\n",
      "param norm: 17.959733963012695\n",
      "\n",
      "Test set: Avg. loss: 0.2517, Accuracy: 9771/10000 (98%)\n",
      "\n",
      "Took 14.783269882202148s\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.173060\n",
      "param norm: 17.99748992919922\n",
      "\n",
      "Test set: Avg. loss: 0.3068, Accuracy: 9664/10000 (97%)\n",
      "\n",
      "Took 14.950882196426392s\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.141323\n",
      "param norm: 17.995868682861328\n",
      "\n",
      "Test set: Avg. loss: 0.2480, Accuracy: 9775/10000 (98%)\n",
      "\n",
      "Took 14.587067127227783s\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.162086\n",
      "param norm: 17.949899673461914\n",
      "\n",
      "Test set: Avg. loss: 0.2294, Accuracy: 9806/10000 (98%)\n",
      "\n",
      "Took 14.645104885101318s\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.150360\n",
      "param norm: 17.946218490600586\n",
      "\n",
      "Test set: Avg. loss: 0.2410, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "Took 14.61369276046753s\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.178591\n",
      "param norm: 17.977848052978516\n",
      "\n",
      "Test set: Avg. loss: 0.2557, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Took 14.650357723236084s\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.138929\n",
      "param norm: 17.926773071289062\n",
      "\n",
      "Test set: Avg. loss: 0.2628, Accuracy: 9763/10000 (98%)\n",
      "\n",
      "Took 14.673272371292114s\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.162195\n",
      "param norm: 17.975074768066406\n",
      "\n",
      "Test set: Avg. loss: 0.2317, Accuracy: 9811/10000 (98%)\n",
      "\n",
      "Took 14.588623285293579s\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.148606\n",
      "param norm: 17.95178985595703\n",
      "\n",
      "Test set: Avg. loss: 0.2853, Accuracy: 9702/10000 (97%)\n",
      "\n",
      "Took 14.721929550170898s\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.130120\n",
      "param norm: 17.94502830505371\n",
      "\n",
      "Test set: Avg. loss: 0.2439, Accuracy: 9779/10000 (98%)\n",
      "\n",
      "Took 14.599579095840454s\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.202841\n",
      "param norm: 17.947751998901367\n",
      "\n",
      "Test set: Avg. loss: 0.2371, Accuracy: 9811/10000 (98%)\n",
      "\n",
      "Took 14.683634519577026s\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.165448\n",
      "param norm: 17.950355529785156\n",
      "\n",
      "Test set: Avg. loss: 0.2514, Accuracy: 9756/10000 (98%)\n",
      "\n",
      "Took 14.611632585525513s\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.172805\n",
      "param norm: 17.954511642456055\n",
      "\n",
      "Test set: Avg. loss: 0.2374, Accuracy: 9800/10000 (98%)\n",
      "\n",
      "Took 14.55430293083191s\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.153613\n",
      "param norm: 17.94778823852539\n",
      "\n",
      "Test set: Avg. loss: 0.2310, Accuracy: 9794/10000 (98%)\n",
      "\n",
      "Took 14.626345157623291s\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.146406\n",
      "param norm: 17.95318031311035\n",
      "\n",
      "Test set: Avg. loss: 0.2223, Accuracy: 9817/10000 (98%)\n",
      "\n",
      "Took 14.597306251525879s\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.206990\n",
      "param norm: 17.958660125732422\n",
      "\n",
      "Test set: Avg. loss: 0.2345, Accuracy: 9808/10000 (98%)\n",
      "\n",
      "Took 14.548829793930054s\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.152402\n",
      "param norm: 17.95581817626953\n",
      "\n",
      "Test set: Avg. loss: 0.2444, Accuracy: 9786/10000 (98%)\n",
      "\n",
      "Took 14.666809797286987s\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.156978\n",
      "param norm: 17.95085906982422\n",
      "\n",
      "Test set: Avg. loss: 0.2498, Accuracy: 9774/10000 (98%)\n",
      "\n",
      "Took 14.751760959625244s\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.199391\n",
      "param norm: 18.006547927856445\n",
      "\n",
      "Test set: Avg. loss: 0.2435, Accuracy: 9782/10000 (98%)\n",
      "\n",
      "Took 15.018497467041016s\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"### MNIST FRAMEWORK K-FAC solver\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from Q_KLD_WRM import Q_KLD_WRM_Optimizer\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor') \n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "torch.backends.cudnn.enabled = True # False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ------------------------------\n",
    "# ---- Training parameters -----\n",
    "n_epochs = 50\n",
    "opti_type = 'Q_KLD_WRM'\n",
    "# l_rate = 0.01;\n",
    "def l_rate_function(epoch_n, iter_n):\n",
    "    if epoch_n == 1:\n",
    "        if iter_n < 3:\n",
    "            return 0.01\n",
    "        else:\n",
    "            return 0.01\n",
    "    elif epoch_n == 2:\n",
    "        return 0.01\n",
    "    elif epoch_n >= 3 and epoch_n < 30:\n",
    "        return 0.01\n",
    "    elif epoch_n >= 30:\n",
    "        return 0.01\n",
    "\n",
    "kfac_clip = 1e-1; KFAC_damping = 1e-02; stat_decay = 0.5\n",
    "\n",
    "WD = 0.001#1\n",
    "lambdaa = 0.0#1#1#7 #0.007\n",
    "batch_size_train = batch_size_test = 512\n",
    "# ONLY FOR SAVED FILE NAME: beta1 and beta2 are just 2 channels for filename ==\n",
    "beta1 = WD\n",
    "beta2 = KFAC_damping\n",
    "# ====================================================\n",
    "KFAC_matrix_update_frequency = 30\n",
    "\n",
    "momentum = 0.0\n",
    "my_clip_threshold = 2.0\n",
    "log_interval = 200 #int(200 *batch_size_train/8192)\n",
    "basic_path = '/content/gdrive/My Drive/P_data/results{}_MNIST'.format(opti_type)\n",
    "error_write_path = '/content/gdrive/My Drive/P_data/Errors/err_{}_MNIST'.format(opti_type)\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#--------------------------- DATA LOADERS -------------------------------------\n",
    "#------------------------------------------------------------------------------\n",
    "def collation_fct(x):\n",
    "  return  tuple(x_.to(cuda0) for x_ in default_collate(x))\n",
    "\n",
    "# Data Normalisation parameters\n",
    "global_data_mean = 0.1307\n",
    "global_data_std = 0.3081\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('../data_lecun', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                       (global_data_mean,), (global_data_std,))\n",
    "                               ])),\n",
    "    batch_size=batch_size_train, shuffle=True, num_workers = 0, collate_fn = collation_fct) # pin_memory=True,\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('../data_lecun', train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])),\n",
    "    batch_size=batch_size_train, shuffle=True, num_workers = 0, collate_fn = collation_fct) # pin_memory=True,\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#-------------------------------- building the NET ----------------------------\n",
    "scale = 1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nodes_dropout=False):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(5, 7, kernel_size=5)\n",
    "        if nodes_dropout == True:\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(7 * 16, 30) # nn.Linear(7 * 16, 30)\n",
    "        self.fc2 = nn.Linear(30, 10) # nn.Linear(30, 10)\n",
    "        #initialize parameters structure to map gradients from parameter format to math format and viceversa\n",
    "        self.parameter_structure = [] #[i.shape for i in self.parameters()]\n",
    "        self.coarser_param_structure = []\n",
    "        for item in self.parameters():\n",
    "            #print('Parameter on device {}'.format(item.device))\n",
    "            item = item.shape\n",
    "            number_of_elements = 1\n",
    "            current_list = []\n",
    "            for j in item:\n",
    "                number_of_elements = number_of_elements * j\n",
    "                current_list.append(j)\n",
    "            if len(current_list) == 1:\n",
    "                current_list.append(1)\n",
    "            self.parameter_structure.append(current_list)\n",
    "            self.coarser_param_structure.append(number_of_elements)\n",
    "        self.even_coarser_param_structure = list(np.array([self.coarser_param_structure[i] for i in range(len(self.coarser_param_structure)) if i % 2 == 1]) + np.array([self.coarser_param_structure[i] for i in range(len(self.coarser_param_structure)) if i % 2 == 0]))\n",
    "        self.number_of_parameters = np.sum(self.coarser_param_structure)\n",
    "\n",
    "    def forward(self, x, nodes_dropout=False):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        if nodes_dropout == True:\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        else:\n",
    "            x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 112)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "network = Net()\n",
    "network.to(cuda0)\n",
    "for p in network.parameters():  print('After moving, parameters on device {}'.format(p.device))\n",
    "#network.load_state_dict(torch.load('./model_0.pth'))\n",
    "print('the number of parameters is {}'.format(sum([p.numel() for p in network.parameters()])))\n",
    "# -------------------------------------------\n",
    "\n",
    "# --------------------------------- DEFINE THE OPTIMISER ----------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "def regularized_loss_fct(output,target,network, lambdaa):\n",
    "    KFAC_matrix_loss = F.cross_entropy(output,target)\n",
    "    l2_reg = torch.tensor(0., device = cuda0)\n",
    "    for param in network.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "    KFAC_matrix_loss += lambdaa * l2_reg\n",
    "    \n",
    "    loss_for_gradient = F.cross_entropy(output,target)\n",
    "    l2_reg = torch.tensor(0., device = cuda0)\n",
    "    for param in network.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "    loss_for_gradient += lambdaa * l2_reg\n",
    "    \n",
    "    #print('the parameter vector norm is {}'.format(l2_reg))\n",
    "    return KFAC_matrix_loss, loss_for_gradient, l2_reg\n",
    "\n",
    "\n",
    "# select types with' ': SGD, ADAM, ADAMAX, NADAM, AMSGRAD, RMSPROP, LBFGS\n",
    "# note that LBFGS computes the same\n",
    "optimizer = Q_KLD_WRM_Optimizer(network, lr_function = l_rate_function, momentum = 0.0, stat_decay = stat_decay, \n",
    "                          kl_clip = kfac_clip, damping = KFAC_damping, weight_decay = WD, Ts = KFAC_matrix_update_frequency,\n",
    "                          Tf = KFAC_matrix_update_frequency, my_clip_threshold = my_clip_threshold)\n",
    "\n",
    "scheduler = None\n",
    "#scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = [lambda1, lambda2])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# track performance of parameters and progress\n",
    "train_losses = []; train_losses_per_epoch = []; train_accuracy_per_epoch = []\n",
    "\n",
    "train_accuracy = []; test_accuracy = []; time_per_epoch_ = []; time_per_iter = []\n",
    "train_counter = []; test_losses = []\n",
    "test_counter = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "##initial network\n",
    "\n",
    "# torch.save(network.state_dict(), './resultsSGD/model.pth')\n",
    "def save_data_():\n",
    "    # basic_path = os.path.join(basic_path, '/')\n",
    "    train_losses_cpu = []; train_losses_per_epoch_cpu = []; train_accuracy_per_epoch_cpu = []\n",
    "    train_accuracy_cpu = []; test_accuracy_cpu = []; time_per_epoch__cpu = []; time_per_iter_cpu = []\n",
    "    test_losses_cpu = []\n",
    "\n",
    "    for trl,tim,trlpe,tracc,traccpe,timpe,teacc,tel in zip(train_losses, time_per_iter, train_losses_per_epoch, train_accuracy,\n",
    "                                                           train_accuracy_per_epoch, time_per_epoch_, \n",
    "                                                           test_accuracy_cpu, test_losses_cpu):\n",
    "      train_losses_cpu.append(trl.cpu());time_per_iter_cpu.append(tim.cpu());\n",
    "      train_losses_per_epoch_cpu.append(trlpe.cpu()); train_accuracy_cpu.append(tracc.cpu());\n",
    "      train_accuracy_per_epoch_cpu.append(traccpe.cpu());  time_per_epoch__cpu.append(timpe.cpu());  test_accuracy_cpu.append(teacc.cpu());\n",
    "      test_losses_cpu.append(tel.cpu());\n",
    "    \n",
    "    np.save(os.path.join(basic_path,\n",
    "                         '{}_{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                    beta1,\n",
    "                                                    beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                    my_clip_threshold, random_seed, 'train_losses')),\n",
    "            train_losses_cpu)\n",
    "    \n",
    "    np.save(os.path.join(basic_path,\n",
    "                         '{}_{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                    beta1,\n",
    "                                                    beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                    my_clip_threshold, random_seed, 'time_per_iter')),\n",
    "            time_per_iter_cpu)\n",
    "\n",
    "    np.save(os.path.join(basic_path,\n",
    "                         '{}_{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                    beta1,\n",
    "                                                    beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                    my_clip_threshold, random_seed, 'train_losses_per_epoch')),\n",
    "            train_losses_per_epoch_cpu)\n",
    "    np.save(os.path.join(basic_path,\n",
    "                         '{}_{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                    beta1,\n",
    "                                                    beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                    my_clip_threshold, random_seed, 'train_accuracy')),\n",
    "            train_accuracy_cpu)\n",
    "    np.save(os.path.join(basic_path,\n",
    "                         '{}_{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                    beta1,\n",
    "                                                    beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                    my_clip_threshold, random_seed, 'train_accuracy_per_epoch')),\n",
    "            train_accuracy_per_epoch_cpu)\n",
    "    np.save(os.path.join(basic_path,\n",
    "                         '{}_{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                    beta1,\n",
    "                                                    beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                    my_clip_threshold, random_seed, 'time_per_epoch_')),\n",
    "            time_per_epoch__cpu)\n",
    "    np.save(os.path.join(basic_path,\n",
    "                         '{}_{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                    beta1,\n",
    "                                                    beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                    my_clip_threshold, random_seed, 'test_accuracy')),\n",
    "            test_accuracy_cpu)\n",
    "    np.save(os.path.join(basic_path,\n",
    "                         '{}_{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                    beta1,\n",
    "                                                    beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                    my_clip_threshold, random_seed, 'test_losses')),\n",
    "            test_losses_cpu)\n",
    "\n",
    "def train(epoch, step_counter, log_interval = log_interval):\n",
    "    network.train()\n",
    "    correct = 0\n",
    "    time_epoch = 0\n",
    "    optimizer.epoch_number = epoch\n",
    "    # previous_step = np.array([0])\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        step_counter = step_counter + 1\n",
    "        start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        #data = data.double()\n",
    "        output = network(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "        KFAC_matrix_loss, loss_for_gradient, l2_reg = regularized_loss_fct(output, target, network, lambdaa)\n",
    "        #t1 = time.time()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ''' assemble <KFAC matrix> loss to compute KFAC matrix'''\n",
    "        if optimizer.steps % KFAC_matrix_update_frequency == 0:\n",
    "            optimizer.acc_stats = True\n",
    "            KFAC_matrix_loss.backward(retain_graph=True)\n",
    "        optimizer.acc_stats = False\n",
    "    \n",
    "        ''' compute gradient of <Policy loss> (precond by KFAC^{-1}) and then take step'''\n",
    "        ''' also need to compute and return the gradient for TRUEish F^{-1}g computation'''\n",
    "        optimizer.zero_grad()\n",
    "        loss_for_gradient.backward()\n",
    "        KFAC_direction = optimizer.step(epoch, error_write_path)\n",
    "        loss_value = loss_for_gradient.detach()\n",
    "        end = time.time()\n",
    "\n",
    "        time_per_iter.append(start - end)\n",
    "        time_epoch = time_epoch + (end - start)\n",
    "        train_losses.append(loss_value)\n",
    "        if scheduler == None:\n",
    "            pass\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            train_counter.append(\n",
    "                (batch_idx * 64) + ((epoch - 1) * len(train_loader.dataset)))\n",
    "            # change the saving path\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss_value))\n",
    "            # print('\\n negative evals steps:{} \\n positive eval steps:{}\\n'.format(negative_eigenvalue_steps,nonnegative_eigenvalue_steps))\n",
    "            torch.save(network.state_dict(), '/content/gdrive/My Drive/P_data/results{}_MNIST/model.pth'.format(opti_type))\n",
    "            torch.save(optimizer.state_dict(), '/content/gdrive/My Drive/P_data/results{}_MNIST/optimizer.pth'.format(opti_type))\n",
    "            print('param norm: {}'.format(l2_reg))\n",
    "\n",
    "            save_data_()\n",
    "\n",
    "            #print('the number {} shoudl be (the number above)*lambdaa '\n",
    "            #      'if reg works'.format(loss.detach()-F.cross_entropy(output,target)))\n",
    "\n",
    "    accc = 100. * correct / len(train_loader.dataset)\n",
    "    train_accuracy.append(accc)\n",
    "    time_per_epoch_.append(time_epoch)\n",
    "    train_losses_per_epoch.append(loss_value)\n",
    "    train_accuracy_per_epoch.append(accc)\n",
    "    save_data_()\n",
    "    return step_counter\n",
    "\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        data_iter = iter(test_loader)\n",
    "        for dummy_index in range(len(test_loader)):\n",
    "            data, target = next(data_iter)\n",
    "            #print('Data on device {}'.format(data.device))\n",
    "            #data = data.double()\n",
    "            output = network(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).detach()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    accc = 100. * correct / len(test_loader.dataset)\n",
    "    test_accuracy.append(accc)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "test()\n",
    "step_counter = 0\n",
    "print('\\n', optimizer.m_aa, '\\n')\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    t1 = time.time()\n",
    "    step_counter = train(epoch, step_counter)\n",
    "    test()\n",
    "    t2 = time.time()\n",
    "    print('Took {}s'.format(t2-t1))\n",
    "print('\\nDone!')\n",
    "save_data = True\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPfmgu0yz0di01mxYkXhpEH",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MNIST_Q_CUDA_TRIAL_manual_cuda0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
